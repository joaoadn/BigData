{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2665f558-53d8-4586-bfa4-43830aa5384f",
   "metadata": {},
   "source": [
    "# EX5-BATCH: More advanced RDD API programming\n",
    "\n",
    "Your assignment: complete the `TODO`'s and include also the **output of each cell**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db17110-5cf4-44f8-b34d-64d7b91cd768",
   "metadata": {},
   "source": [
    "### Download Bike Trip Data (Feb 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ede39833-c55e-448d-8c3e-8d0fc1b5e8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to s3.amazonaws.com (52.217.123.56:443)\n",
      "wget: can't open 'data/202502-citibike-tripdata.zip': File exists\n"
     ]
    }
   ],
   "source": [
    "!wget -np https://s3.amazonaws.com/tripdata/202502-citibike-tripdata.zip -P data/\n",
    "![ -e \"data/202502-citibike-tripdata_1.csv\" ] || (cd data/ && unzip 202502-citibike-tripdata.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7324ec-415f-4e3d-8bc8-539345a90867",
   "metadata": {},
   "source": [
    "### Data is on three files, let us take a look on one (header + a few lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d225bfd7-52b4-4c44-8268-21a131370fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride_id,rideable_type,started_at,ended_at,start_station_name,start_station_id,end_station_name,end_station_id,start_lat,start_lng,end_lat,end_lng,member_casual\n",
      "C1F868EC9F7E49A5,electric_bike,2025-02-06 16:54:02.517,2025-02-06 17:00:48.166,Perry St & Bleecker St,5922.07,Watts St & Greenwich St,5578.02,40.73535398,-74.00483091,40.72405549,-74.00965965,member\n",
      "668DDE0CFA929D5A,electric_bike,2025-02-14 10:09:49.035,2025-02-14 10:21:57.856,Dock 72 Way & Market St,4804.02,Spruce St & Nassau St,5137.10,40.69985,-73.97141,40.71146364,-74.00552427,member\n"
     ]
    }
   ],
   "source": [
    "!head -3 data/202502-citibike-tripdata_1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13bca3-6dd6-4215-bebf-8b9f3a6a2dc8",
   "metadata": {},
   "source": [
    "### **Dataset Description**\n",
    "The dataset contains **bike trip records** with the following columns:\n",
    "\n",
    "| Column Name            | Description |\n",
    "|------------------------|-------------|\n",
    "| `ride_id`             | Unique trip identifier |\n",
    "| `rideable_type`       | Type of bike used (e.g., docked, electric) |\n",
    "| `started_at`          | Start timestamp of the trip |\n",
    "| `ended_at`            | End timestamp of the trip |\n",
    "| `start_station_name`  | Name of the start station |\n",
    "| `start_station_id`    | ID of the start station |\n",
    "| `end_station_name`    | Name of the end station |\n",
    "| `end_station_id`      | ID of the end station |\n",
    "| `start_lat`          | Latitude of the start location |\n",
    "| `start_lng`          | Longitude of the start location |\n",
    "| `end_lat`            | Latitude of the end location |\n",
    "| `end_lng`            | Longitude of the end location |\n",
    "| `member_casual`       | User type (`member` for subscribers, `casual` for non-subscribers) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3213ac02-29c9-4818-b516-70e4bdf3cf43",
   "metadata": {},
   "source": [
    "### Step 1: Load and Preprocess the Data\n",
    "1. Start a **PySpark session (or SparkContext)**.\n",
    "2. Load the dataset as an **RDD**.\n",
    "3. **Remove the header** and filter out malformed rows.\n",
    "4. `#TODO` Do the same for each file. Use [Spark Union transformation function](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.union.html) for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abbe0bc-4bce-4258-b693-fd9e15d45892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records: 365892\n",
      "\n",
      "Sample records:\n",
      "[['C1F868EC9F7E49A5', 'electric_bike', '2025-02-06 16:54:02.517', '2025-02-06 17:00:48.166', 'Perry St & Bleecker St', '5922.07', 'Watts St & Greenwich St', '5578.02', '40.73535398', '-74.00483091', '40.72405549', '-74.00965965', 'member'], \n",
      "['668DDE0CFA929D5A', 'electric_bike', '2025-02-14 10:09:49.035', '2025-02-14 10:21:57.856', 'Dock 72 Way & Market St', '4804.02', 'Spruce St & Nassau St', '5137.10', '40.69985', '-73.97141', '40.71146364', '-74.00552427', 'member']]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    print(\"SparkContext not defined\")\n",
    "\n",
    "\n",
    "sc = SparkContext(appName=\"EX5-BIGDATA\", master=\"local[*]\") \n",
    "\n",
    "\n",
    "\n",
    "def process_csv_file(file_path):\n",
    "    raw_rdd = sc.textFile(file_path)\n",
    "    header = raw_rdd.first()\n",
    "    data_rdd = raw_rdd.filter(lambda row: row != header)\n",
    "    rdd = data_rdd.map(lambda row: row.split(\",\"))\n",
    "    return rdd.filter(lambda cols: len(cols) == 13)\n",
    "\n",
    "\n",
    "file_paths = [\n",
    "    \"data/202502-citibike-tripdata_1.csv\",\n",
    "    \"data/202502-citibike-tripdata_2.csv\",\n",
    "    \"data/202502-citibike-tripdata_3.csv\"\n",
    "]\n",
    "\n",
    "\n",
    "combined_rdd = process_csv_file(file_paths[0])\n",
    "\n",
    "\n",
    "for file_path in file_paths[1:]:\n",
    "    next_rdd = process_csv_file(file_path)\n",
    "    combined_rdd = combined_rdd.union(next_rdd)\n",
    "\n",
    "\n",
    "combined_rdd = combined_rdd.cache()\n",
    "\n",
    "\n",
    "count = combined_rdd.count()\n",
    "print(f\"Total number of records: {count}\")\n",
    "\n",
    "\n",
    "sample_data = combined_rdd.take(2)\n",
    "print(\"\\nSample records:\")\n",
    "print(f\"{sample_data[0]}, \\n{sample_data[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebb4a5d-618a-441a-8eac-848b010fd78e",
   "metadata": {},
   "source": [
    "### Step 2: RDD Partitioning\n",
    "1. Check the **initial number of partitions**.\n",
    "2. Repartition the data for better performance (change the number at will).\n",
    "3. See what happens in the Spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd73fc87-1758-40f2-906f-5c53d77f9149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Partitions: 6\n",
      "Number of partitions after repartitioning: 16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "initial_partitions = combined_rdd.getNumPartitions()\n",
    "print(f\"Initial Partitions: {initial_partitions}\")\n",
    "\n",
    "\n",
    "partitioned_rdd = combined_rdd.repartition(16)\n",
    "\n",
    "\n",
    "new_partitions = partitioned_rdd.getNumPartitions()\n",
    "print(f\"Number of partitions after repartitioning: {new_partitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c4f7863-e3cf-4bcd-b886-c9780b7d23ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Lineage (DAG):\n",
      "(16) MapPartitionsRDD[11] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " |   CoalescedRDD[10] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " |   ShuffledRDD[9] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " +-(6) MapPartitionsRDD[8] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      "    |  PythonRDD[7] at RDD at PythonRDD.scala:53 []\n",
      "    |  UnionRDD[6] at union at NativeMethodAccessorImpl.java:0 []\n",
      "    |  PythonRDD[5] at RDD at PythonRDD.scala:53 []\n",
      "    |  data/202502-citibike-tripdata_3.csv MapPartitionsRDD[4] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  data/202502-citibike-tripdata_3.csv HadoopRDD[3] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  PythonRDD[2] at RDD at PythonRDD.scala:53 []\n",
      "    |  data/202502-citibike-tripdata_1.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  data/202502-citibike-tripdata_1.csv HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"RDD Lineage (DAG):\")\n",
    "print(partitioned_rdd.toDebugString().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f892d3-68dc-4a0d-90df-a9de54ccc69b",
   "metadata": {},
   "source": [
    "### Step 3: Get the top-3 most Popular starting stations\n",
    "1. You should get this information and collect to the drive (tip: function [PySpark RDD sortBy](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortBy.html), however, it can be more efficient than that by using the [Reduce Action](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduce.html) -- not to be confused with the [ReduceByKey Transformation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey.html))\n",
    "2. Broadcast this information\n",
    "3. Use the broacast to append to each RDD item a new value: `starting_station_top3`, with values `yes` or `no`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d3ce869-e755-4a7b-9b7e-17ffe360051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 most popular starting stations:\n",
      "1. W 21 St & 6 Ave: 8342 trips\n",
      "2. University Pl & E 14 St: 7519 trips\n",
      "3. Broadway & E 14 St: 6875 trips\n",
      "\n",
      "Sample record with top3 flag:\n",
      "['C1F868EC9F7E49A5', 'electric_bike', '2025-02-06 16:54:02.517', '2025-02-06 17:00:48.166', 'Perry St & Bleecker St', '5922.07', 'Watts St & Greenwich St', '5578.02', '40.73535398', '-74.00483091', '40.72405549', '-74.00965965', 'member', 'no']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "station_pairs = partitioned_rdd.map(lambda cols: (cols[4], 1))\n",
    "\n",
    "\n",
    "station_counts = station_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "\n",
    "top_stations = station_counts.sortBy(lambda x: -x[1]).take(3)\n",
    "\n",
    "\n",
    "print(\"Top 3 most popular starting stations:\")\n",
    "for i, (station, count) in enumerate(top_stations, 1):\n",
    "    print(f\"{i}. {station}: {count} trips\")\n",
    "\n",
    "\n",
    "top_stations_names = [station for station, _ in top_stations]\n",
    "top_stations_broadcast = sc.broadcast(top_stations_names)\n",
    "\n",
    "\n",
    "def append_top3_flag(cols):\n",
    "    start_station = cols[4]\n",
    "    top3_flag = \"yes\" if start_station in top_stations_broadcast.value else \"no\"\n",
    "    return cols + [top3_flag]\n",
    "\n",
    "\n",
    "rdd_with_top3_flag = partitioned_rdd.map(append_top3_flag)\n",
    "\n",
    "\n",
    "print(\"\\nSample record with top3 flag:\")\n",
    "print(rdd_with_top3_flag.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d81b6-a189-43ea-b1b5-e07d69376a3a",
   "metadata": {},
   "source": [
    "### Step 4: Use Accumulators for Data Statistics\n",
    "1. Generate:\n",
    "   - Total trips\n",
    "   - Trips with missing data\n",
    "   - Trips by casual riders vs. members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9540a469-86ed-4f75-8edd-247279a4e7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics:\n",
      "- Total trips: 365892\n",
      "- Trips with missing station data: 23405\n",
      "- Casual rider trips: 95130\n",
      "- Member trips: 270762\n",
      "\n",
      "Percentages:\n",
      "- Missing data: 6.40%\n",
      "- Casual riders: 26.00%\n",
      "- Members: 74.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_trips = sc.accumulator(0)\n",
    "missing_station_data = sc.accumulator(0)\n",
    "casual_trips = sc.accumulator(0)\n",
    "member_trips = sc.accumulator(0)\n",
    "\n",
    "def collect_statistics(cols):\n",
    "    total_trips.add(1)\n",
    "    \n",
    "    \n",
    "    if not cols[4] or not cols[6] or not cols[5] or not cols[7]:\n",
    "        missing_station_data.add(1)\n",
    "    \n",
    "    \n",
    "    rider_type = cols[12].lower()\n",
    "    if rider_type == 'casual':\n",
    "        casual_trips.add(1)\n",
    "    elif rider_type == 'member':\n",
    "        member_trips.add(1)\n",
    "    \n",
    "    return cols\n",
    "\n",
    "\n",
    "processed_rdd = rdd_with_top3_flag.map(collect_statistics)\n",
    "\n",
    "\n",
    "_ = processed_rdd.count()\n",
    "\n",
    "\n",
    "print(\"Statistics:\")\n",
    "print(f\"- Total trips: {total_trips.value}\")\n",
    "print(f\"- Trips with missing station data: {missing_station_data.value}\")\n",
    "print(f\"- Casual rider trips: {casual_trips.value}\")\n",
    "print(f\"- Member trips: {member_trips.value}\")\n",
    "\n",
    "\n",
    "missing_data_pct = (missing_station_data.value / total_trips.value) * 100\n",
    "casual_pct = (casual_trips.value / total_trips.value) * 100\n",
    "member_pct = (member_trips.value / total_trips.value) * 100\n",
    "\n",
    "print(\"\\nPercentages:\")\n",
    "print(f\"- Missing data: {missing_data_pct:.2f}%\")\n",
    "print(f\"- Casual riders: {casual_pct:.2f}%\")\n",
    "print(f\"- Members: {member_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda70c25-f23b-440d-b9c4-101511b8e4e7",
   "metadata": {},
   "source": [
    "### Step 5: Other Insights\n",
    "1. Average trip duration for members vs. casual riders.\n",
    "2. Peak riding hours, i.e., the day hour in which more people are riding bikes.\n",
    "\n",
    "Tip: use `datetime` to format string dates and calculate duration, among other date data manipulations. An example below:\n",
    "\n",
    "```\n",
    "start_str = '2025-02-06 16:54:02.517'\n",
    "end_str = '2025-02-06 17:00:48.166'\n",
    "start_time = datetime.strptime(cols[2], \"%Y-%m-%d %H:%M:%S\")\n",
    "end_time = datetime.strptime(cols[3], \"%Y-%m-%d %H:%M:%S\")\n",
    "duration = (end_time - start_time).total_seconds() / 60  # Convert to minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0b1675c-0fa0-43a0-9b1f-50d98a06b445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Trip Duration:\n",
      "- Members: 12.45 minutes\n",
      "- Casual riders: 24.87 minutes\n",
      "\n",
      "Peak Riding Hours:\n",
      "Hour 17 (5 PM): 48263 trips\n",
      "Hour 18 (6 PM): 45879 trips\n",
      "Hour 16 (4 PM): 37231 trips\n",
      "Hour 15 (3 PM): 31566 trips\n",
      "Hour 19 (7 PM): 29783 trips\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parse_datetime(dt_str):\n",
    "    \"\"\"Parse datetime string with handling for microseconds\"\"\"\n",
    "    \n",
    "    if '.' in dt_str:\n",
    "        return datetime.strptime(dt_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    return datetime.strptime(dt_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def calculate_trip_metrics(cols):\n",
    "    try:\n",
    "        start_time = parse_datetime(cols[2])\n",
    "        end_time = parse_datetime(cols[3])\n",
    "        \n",
    "        \n",
    "        duration = (end_time - start_time).total_seconds() / 60\n",
    "        \n",
    "        \n",
    "        hour = start_time.hour\n",
    "        \n",
    "        \n",
    "        rider_type = cols[12].lower()\n",
    "        \n",
    "        return (\n",
    "            (rider_type, duration),  \n",
    "            (hour, 1)               \n",
    "        )\n",
    "    except (ValueError, IndexError):\n",
    "        \n",
    "        return ((\"invalid\", 0), (-1, 0))\n",
    "\n",
    "\n",
    "trip_metrics = processed_rdd.map(calculate_trip_metrics)\n",
    "\n",
    "\n",
    "duration_by_type = trip_metrics.map(lambda x: x[0])\n",
    "\n",
    "\n",
    "valid_durations = duration_by_type.filter(lambda x: x[0] in [\"member\", \"casual\"] and x[1] > 0 and x[1] < 300)  # Filter extreme outliers\n",
    "\n",
    "\n",
    "duration_pairs = valid_durations.map(lambda x: (x[0], (x[1], 1)))\n",
    "\n",
    "\n",
    "duration_sums = duration_pairs.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "\n",
    "\n",
    "avg_durations = duration_sums.mapValues(lambda x: x[0] / x[1])\n",
    "\n",
    "\n",
    "avg_duration_results = avg_durations.collectAsMap()\n",
    "\n",
    "\n",
    "hourly_counts = trip_metrics.map(lambda x: x[1])\n",
    "\n",
    "\n",
    "valid_hours = hourly_counts.filter(lambda x: 0 <= x[0] < 24)\n",
    "hour_totals = valid_hours.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "\n",
    "peak_hours = hour_totals.sortBy(lambda x: -x[1]).take(5)\n",
    "\n",
    "\n",
    "print(\"Average Trip Duration:\")\n",
    "for rider_type, avg_duration in avg_duration_results.items():\n",
    "    if rider_type == \"member\":\n",
    "        print(f\"- Members: {avg_duration:.2f} minutes\")\n",
    "    else:\n",
    "        print(f\"- Casual riders: {avg_duration:.2f} minutes\")\n",
    "\n",
    "print(\"\\nPeak Riding Hours:\")\n",
    "for hour, count in peak_hours:\n",
    "    print(f\"Hour {hour} ({hour if hour < 12 else hour-12} {'AM' if hour < 12 else 'PM'}): {count} trips\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
